{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpatioTemporalNorm_Experiments.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kvwK25lPTlGv"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/TorchSpatiotemporal/tsl.git\n",
        "!pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torch-scatter torch-sparse torch-geometric -f https://data.pyg.org/whl/torch-1.10.1+cu113.html\n",
        "!pip install ./tsl\n",
        "!pip install import-ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "USGL74x1U4er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tsl\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from tsl.datasets import PemsBay\n",
        "from tsl.datasets import AirQuality\n",
        "from tsl.datasets import MetrLA\n",
        "\n",
        "from tsl.data import SpatioTemporalDataset\n",
        "from tsl.data import SpatioTemporalDataModule\n",
        "from tsl.data.preprocessing import StandardScaler\n",
        "from torch.nn import Parameter, Linear\n",
        "from torch_geometric.nn import inits\n",
        "from einops import rearrange\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch_scatter import scatter_mean\n",
        "\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "from tsl.nn.utils import casting\n",
        "from tsl.utils import TslExperiment, ArgParser, parser_utils, numpy_metrics\n",
        "from tsl.utils.neptune_utils import TslNeptuneLogger\n",
        "from tsl.nn.layers.norm import Norm\n",
        "\n",
        "from tsl.utils.parser_utils import ArgParser, str_to_bool\n",
        "from einops import repeat\n",
        "\n",
        "from tsl.nn.blocks.encoders import ConditionalBlock\n",
        "\n",
        "from tsl.nn.utils.utils import get_layer_activation\n",
        "from tsl.nn.ops.ops import Lambda\n",
        "\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from tsl.nn.metrics.metrics import MaskedMAE, MaskedMAPE, MaskedMSE\n",
        "from tsl.predictors import Predictor\n",
        "\n",
        "from tsl.nn.blocks.encoders import RNN\n",
        "from tsl.nn.blocks.decoders import GCNDecoder\n",
        "from tsl.nn.blocks.encoders.tcn import TemporalConvNet\n",
        "from tsl.nn.base.embedding import StaticGraphEmbedding\n",
        "from tsl.nn.blocks.decoders.mlp_decoder import MLPDecoder\n",
        "from tsl.nn.layers.graph_convs.diff_conv import DiffConv\n",
        "from tsl.nn.layers.graph_convs.dense_spatial_conv import SpatialConvOrderK\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from google.colab import drive\n",
        "import import_ipynb\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "tsl.logger.disabled = True\n",
        "\n",
        "print(f\"tsl version  : {tsl.__version__}\")\n",
        "print(f\"torch version: {torch.__version__}\")"
      ],
      "metadata": {
        "id": "ZIgQ4-XnU2AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive to SpatioTemporalNorm file\n",
        "drive.mount(\"mnt\")"
      ],
      "metadata": {
        "id": "bX0fYTDEV3Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"mnt/My Drive/Colab Notebooks/GDL/\"\n",
        "import SpatioTemporalModel as SpatioTemporalModel"
      ],
      "metadata": {
        "id": "7nfI3N2YbOVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatioTemporalNormExperiment():\n",
        "    norm_weights = []                                                           #Store the normalisation strategy weights\n",
        "\n",
        "    def __init__(self, dataset=\"\", nn_model=\"\", norm=\"united\", tsl_log_version=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.datset = None\n",
        "        self.model = None\n",
        "        self.max_epochs = 10\n",
        "        self.nn_model = nn_model\n",
        "        self.tsl_log_version = tsl_log_version\n",
        "        self.norm = norm\n",
        "\n",
        "        #Validate Dataset\n",
        "        if (dataset == \"MetrLA\"):\n",
        "            self.dataset = MetrLA()\n",
        "        elif (dataset == \"AirQuality\"):\n",
        "            self.dataset = AirQuality()\n",
        "        elif (dataset == \"PemsBay\"):\n",
        "            self.dataset = PemsBay()\n",
        "        else:\n",
        "            raise ValueError(\"Please choose one of the following Datasets: MetrLA, AirQuality, PemsBay\")\n",
        "\n",
        "        #Validate Model and specify args\n",
        "        if (nn_model == \"Time_and_Space\"):\n",
        "            self.model = SpatioTemporalModel.TimeThenSpaceModel\n",
        "            self.model_kwargs = {\n",
        "                \"hidden_size\": 32,\n",
        "                \"rnn_layers\": 1,\n",
        "                \"gcn_layers\": 2,\n",
        "                \"norm\": norm,\n",
        "                \"caller_class\": SpatioTemporalNormExperiment\n",
        "            }\n",
        "            if (self.dataset == \"PemsBay\"):\n",
        "              self.max_epochs = 200                                             #PemsBay dataset is bigger and takes longer to run\n",
        "\n",
        "        elif (nn_model == \"GWNET\"):\n",
        "            self.model = SpatioTemporalModel.GWNETModel\n",
        "            self.model_kwargs = {\n",
        "                \"horizon\": 12,\n",
        "                \"exog_size\": 0,\n",
        "                \"hidden_size\": 32,\n",
        "                \"ff_size\": 64,\n",
        "                \"n_layers\": 4,\n",
        "                \"dropout\": 0.2,\n",
        "                \"temporal_kernel_size\": 2,\n",
        "                \"spatial_kernel_size\": 2,\n",
        "                \"dilation\": 2,\n",
        "                \"dilation_mod\": 2,\n",
        "                \"learned_adjacency\": True,\n",
        "                \"norm\": norm,\n",
        "                \"caller_class\": SpatioTemporalNormExperiment\n",
        "            }\n",
        "            self.max_epochs = 150\n",
        "\n",
        "        elif (nn_model == \"TCN\"):\n",
        "            self.model = SpatioTemporalModel.TCNModel\n",
        "            self.model_kwargs = {\n",
        "                \"horizon\": 12,\n",
        "                \"exog_size\": 0,\n",
        "                \"hidden_size\": 32,\n",
        "                \"ff_size\": 64,\n",
        "                \"n_layers\": 4,\n",
        "                \"dropout\": 0.1,\n",
        "                \"kernel_size\": 2,\n",
        "                \"n_convs_layer\": 2,\n",
        "                \"dilation\": 2,\n",
        "                \"resnet\": True,\n",
        "                \"norm\": norm,\n",
        "                \"caller_class\": SpatioTemporalNormExperiment\n",
        "            }\n",
        "            self.max_epochs = 200\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Please choose one of the following Models: Time_and_Space, GWNET, TCN\")            \n",
        "\n",
        "\n",
        "        #Run 1/3 of original epochs if model is PemsBay as runtime increases significnatly\n",
        "        if (self.dataset == \"PemsBay\"):\n",
        "            self.max_epochs /= 3\n",
        "\n",
        "        self.init_dataset(self.dataset)                                         #Setup dataset\n",
        "\n",
        "\n",
        "    def init_dataset(self, dataset):\n",
        "        adj = dataset.get_connectivity(threshold=0.1,\n",
        "                                include_self=False,\n",
        "                                normalize_axis=1,\n",
        "                                layout=\"edge_index\")\n",
        "    \n",
        "\n",
        "        torch_dataset = SpatioTemporalDataset(*dataset.numpy(return_idx=True),\n",
        "                                    connectivity=adj,\n",
        "                                    mask=dataset.mask,\n",
        "                                    horizon=12,\n",
        "                                    window=12)\n",
        "        \n",
        "        scalers = {'data': StandardScaler(axis=(0, 1))}\n",
        "\n",
        "        splitter = dataset.get_splitter(val_len=0.1, test_len=0.2)\n",
        "\n",
        "        self.dm = SpatioTemporalDataModule(\n",
        "            dataset=torch_dataset,\n",
        "            scalers=scalers,\n",
        "            splitter=splitter,\n",
        "            batch_size=64,\n",
        "        )\n",
        "\n",
        "        self.dm.setup()\n",
        "\n",
        "        if (self.nn_model == \"Time_and_Space\"):\n",
        "            self.model_kwargs['input_size'] = self.dm.n_channels\n",
        "            self.model_kwargs['horizon'] = self.dm.horizon\n",
        "        elif (self.nn_model == \"TCN\"):\n",
        "            self.model_kwargs[\"input_size\"] = self.dm.n_channels\n",
        "            self.model_kwargs[\"output_size\"] = self.dm.n_channels\n",
        "        else:\n",
        "            self.model_kwargs[\"n_nodes\"] = self.dm.n_nodes\n",
        "            self.model_kwargs[\"input_size\"] = self.dm.n_channels\n",
        "            self.model_kwargs[\"output_size\"] = self.dm.n_channels\n",
        "\n",
        "\n",
        "    def plot_norm_weights(self):\n",
        "        list_norm_weights = [[x.item() for x in y] for y in SpatioTemporalNormExperiment.norm_weights]\n",
        "        list_norm_weights = np.array(list_norm_weights)\n",
        "\n",
        "        #Take the mean weights along steps in one forward pass\n",
        "        if (self.nn_model != \"Time_and_Space\"):\n",
        "            reg_norm_weights = []\n",
        "            temp_w = []\n",
        "\n",
        "            for i, w in enumerate(list_norm_weights):\n",
        "                temp_w.append(w.tolist())\n",
        "                if (i == 0 or i % 4 == 0):\n",
        "                    layer_mean_weights = np.mean(temp_w, axis=0, keepdims=True).tolist()\n",
        "                    reg_norm_weights.append(layer_mean_weights[0])\n",
        "                    temp_w = []\n",
        "\n",
        "            list_norm_weights = np.array(reg_norm_weights)\n",
        "\n",
        "        fig = plt.figure()\n",
        "        ax = plt.axes()\n",
        "        fig.set_figheight(10)\n",
        "        fig.set_figwidth(20)\n",
        "        plt.plot(range(len(list_norm_weights)), list_norm_weights[:,0], color='blue', label=\"Batch\", linestyle='-')\n",
        "        plt.plot(range(len(list_norm_weights)), list_norm_weights[:,1], color='g', label=\"Instance\", linestyle='-')\n",
        "        plt.plot(range(len(list_norm_weights)), list_norm_weights[:,2], color='0.75', label=\"Layer\", linestyle=\"-\")\n",
        "        plt.plot(range(len(list_norm_weights)), list_norm_weights[:,3], color='#FFDD44', label=\"Graph\", linestyle='-')\n",
        "        \n",
        "        if (self.norm == \"united_temporal\"):\n",
        "            plt.plot(range(len(list_norm_weights)), list_norm_weights[:,4], color=(1.0,0.2,0.3), label=\"Temporal\", linestyle=\"-\")\n",
        "\n",
        "        plt.legend()\n",
        "\n",
        "    def run(self):\n",
        "        loss_fn = MaskedMAE(compute_on_step=True)                               #Mean Absolute Error = MAE\n",
        "\n",
        "        metrics = {'mae': MaskedMAE(compute_on_step=False),\n",
        "                'mape': MaskedMAPE(compute_on_step=False),                      # MAPE = Mean Absolute Percetage Error\n",
        "                'mae_at_15': MaskedMAE(compute_on_step=False, at=2),            # `2` indicated the third time step,\n",
        "                                                                                # which correspond to 15 minutes ahead\n",
        "                'mae_at_30': MaskedMAE(compute_on_step=False, at=5),\n",
        "                'mae_at_60': MaskedMAE(compute_on_step=False, at=11), }\n",
        "\n",
        "        #Setup predictor\n",
        "        predictor = Predictor(\n",
        "            model_class=self.model,\n",
        "            model_kwargs=self.model_kwargs,\n",
        "            optim_class=torch.optim.Adam,\n",
        "            optim_kwargs={'lr': 0.001},\n",
        "            loss_fn=loss_fn,\n",
        "            metrics=metrics\n",
        "        )\n",
        "\n",
        "        logger = TensorBoardLogger(save_dir=\"logs\", name=\"tsl_intro\", version=self.tsl_log_version)\n",
        "\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            dirpath='logs',\n",
        "            save_top_k=1,\n",
        "            monitor='val_mae',\n",
        "            mode='min',\n",
        "        )\n",
        "\n",
        "        trainer = pl.Trainer(max_epochs=self.max_epochs,\n",
        "                            logger=logger,\n",
        "                            gpus=1 if torch.cuda.is_available() else None,\n",
        "                            callbacks=[checkpoint_callback])\n",
        "\n",
        "        trainer.fit(predictor, datamodule=self.dm)\n",
        "\n",
        "        #Plot the normalistion strategy weight evolution if using a variation of united norm\n",
        "        if (self.norm in [\"united\", \"united_temporal\"]):\n",
        "            self.plot_norm_weights()\n"
      ],
      "metadata": {
        "id": "XkpXT9tOU7GA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tensor Board Visualisation of Metrics\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "jW3Fl_EWXx2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate an experiment\n",
        "chird = SpatioTemporalNormExperiment(dataset=\"AirQuality\", nn_model=\"Time_and_Space\", norm=\"united_temporal\", tsl_log_version=1)"
      ],
      "metadata": {
        "id": "VzM-RpcXX5i-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run the experiement\n",
        "chird.run()"
      ],
      "metadata": {
        "id": "r00WG2yFYCR-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}