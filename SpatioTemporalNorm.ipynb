{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fEAzpiZL2oG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.nn import inits\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "from tsl.nn.layers.norm import Norm\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h5W6NxpCbce"
      },
      "outputs": [],
      "source": [
        "class GraphNorm(torch.nn.Module):\n",
        "    \"\"\"Adapted GraphNorm from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/norm/graph_norm.html#GraphNorm\n",
        "    \n",
        "    Applies graph normalization over individual graphs as described in the\n",
        "    `\"GraphNorm: A Principled Approach to Accelerating Graph Neural Network\n",
        "    Training\" <https://arxiv.org/abs/2009.03294>`_ paper\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        eps (float, optional): A value added to the denominator for numerical\n",
        "            stability. (default: :obj:`1e-5`)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, eps: float = 1e-5, affine: bool = True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.eps = eps\n",
        "\n",
        "        if affine:\n",
        "            self.weight = Parameter(torch.Tensor(in_channels))\n",
        "            self.bias = Parameter(torch.Tensor(in_channels))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        #Param that learns how much information to retain in the mean\n",
        "        self.mean_scale = torch.nn.Parameter(torch.Tensor(in_channels))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        inits.ones(self.weight)\n",
        "        inits.zeros(self.bias)\n",
        "        inits.ones(self.mean_scale)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        #x = [batch step node channel/feature]\n",
        "        step_dim = 0\n",
        "        batch = x.new_zeros(x.size(step_dim), dtype=torch.long)\n",
        "        mean = torch.mean(x, dim=step_dim, keepdim=True)\n",
        "        x = x - mean.index_select(step_dim, batch) * self.mean_scale\n",
        "        std = torch.std(x, dim=step_dim, unbiased=False, keepdim=True)\n",
        "        \n",
        "        out = x / (std + self.eps)\n",
        "\n",
        "        if self.weight is not None and self.bias is not None:\n",
        "            return self.weight * out + self.bias\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}({self.in_channels})'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JwnEjtvKlmZ"
      },
      "outputs": [],
      "source": [
        "class TemporalNorm(torch.nn.Module):\n",
        "    \"\"\"Applies normalisation over the temporal dimension of a spatio-temporal\n",
        "    input tensor (along time-steps).\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        eps (float, optional): A value added to the denominator for numerical\n",
        "            stability. (default: :obj:`1e-5`)\n",
        "        affine (bool, optional): If set to :obj:`True`, this module has\n",
        "            learnable affine parameters :math:`\\gamma` and :math:`\\beta`.\n",
        "            (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, eps=1e-5, affine=True, mean_lr=0.00001, gate_lr=0.001, scale_lr=0.00001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.eps = eps\n",
        "        # self.spatial_norm = InstanceBatchLayerGraphNorm(in_channels, affine=False)\n",
        "\n",
        "        if affine:\n",
        "            self.weight = Parameter(torch.Tensor(in_channels))\n",
        "            self.bias = Parameter(torch.Tensor(in_channels))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        inits.ones(self.weight)\n",
        "        inits.zeros(self.bias)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # x : [*, steps, nodes, features]\n",
        "        mean = torch.mean(x, dim=-3, keepdim=True)\n",
        "        std = torch.std(x, dim=-3, unbiased=False, keepdim=True)\n",
        "        out = (x - mean) / (std + self.eps)   #Temporal norm\n",
        "\n",
        "        if self.weight is not None and self.bias is not None:\n",
        "            out = out * self.weight + self.bias\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}({self.in_channels})'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxkkPXOvHsLu"
      },
      "outputs": [],
      "source": [
        "class UnitedNorm(torch.nn.Module):\n",
        "    \"\"\"Adapted from https://github.com/cyh1112/GraphNormalization/blob/master/norm/united_norm.py\n",
        "\n",
        "    Applies a united spatial norm on the input by combining and weighting different normalization\n",
        "    strategies on the given input, as stated in the `Learning Graph Normalization for Graph Neural\n",
        "    Networks <https://arxiv.org/pdf/2009.11746.pdf>` paper.\n",
        "\n",
        "    This variation of the idea encompasses Instance Norm instead of the originally specified \n",
        "    Adjacent-wise normalisation.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        affine (bool, optional): If set to :obj:`True`, this module has\n",
        "            learnable affine parameters :math:`\\gamma` and :math:`\\beta`.\n",
        "            (default: :obj:`True`)\n",
        "        caller_class (SpatioTemporalNormExperiment, optional): caller class to store norm weights\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, caller_class, affine=True):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.caller_class = caller_class\n",
        "\n",
        "        if (affine):\n",
        "            self.weight = Parameter(torch.ones(self.in_channels))\n",
        "            self.bias = Parameter(torch.zeros(self.in_channels))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        #Trainable gate parameters that indicate the contribution of a normalization strategy \n",
        "        self.lambda_batch = Parameter(torch.ones(self.in_channels))\n",
        "        self.lambda_instance = Parameter(torch.ones(self.in_channels))\n",
        "        self.lambda_layer = Parameter(torch.ones(self.in_channels))\n",
        "        self.lambda_graph = Parameter(torch.ones(self.in_channels))\n",
        "\n",
        "        self.batch_norm = self.batch_norm = Norm(norm_type=\"batch\", in_channels=self.in_channels, affine=False)\n",
        "        self.instance_norm = Norm(norm_type=\"instance\", in_channels=self.in_channels, affine=False)\n",
        "        self.layer_norm = Norm(norm_type=\"layer\", in_channels=self.in_channels, affine=False)\n",
        "        self.graph_norm = GraphNorm(self.in_channels, affine=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lambda_softmax = F.softmax(torch.cat([self.lambda_batch.unsqueeze(0), self.lambda_instance.unsqueeze(0), self.lambda_layer.unsqueeze(0), self.lambda_graph.unsqueeze(0)], dim=0), dim=0)\n",
        "        x = lambda_softmax[0]*self.batch_norm(x) + lambda_softmax[1]*self.instance_norm(x) + lambda_softmax[2]*self.layer_norm(x) + lambda_softmax[3]*self.graph_norm(x)\n",
        "        \n",
        "        if self.weight is not None and self.bias is not None:\n",
        "            x = x * self.weight + self.bias\n",
        "\n",
        "        #Append each norm's weighting at every step to track evolution\n",
        "        self.caller_class.norm_weights.append(list(lambda_softmax.mean(-1)))  #Store the mean weight (across features) for each normalisation strategy\n",
        "\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}({self.in_channels})'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAO7tYiMFtx3"
      },
      "outputs": [],
      "source": [
        "class UnitedTemporalNorm(torch.nn.Module):\n",
        "    \"\"\"Applies a united norm using spatial and temporal normalization strategies\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        affine (bool, optional): If set to :obj:`True`, this module has\n",
        "            learnable affine parameters :math:`\\gamma` and :math:`\\beta`.\n",
        "            (default: :obj:`True`)\n",
        "        caller_class (SpatioTemporalNormExperiment, optional): caller class to store norm weights\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, caller_class, affine=True):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.caller_class = caller_class\n",
        "\n",
        "        if (affine):\n",
        "            self.weight = Parameter(torch.ones(self.in_channels))\n",
        "            self.bias = Parameter(torch.zeros(self.in_channels))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.lambda_batch = Parameter(torch.ones(self.in_channels))\n",
        "        self.lambda_instance = Parameter(torch.ones(self.in_channels))\n",
        "        self.lambda_layer = Parameter(torch.ones(self.in_channels))\n",
        "        self.lambda_graph = Parameter(torch.ones(self.in_channels))\n",
        "        self.lambda_temporal = Parameter(torch.ones(self.in_channels))\n",
        "        self.lambda_sptpl = Parameter(torch.ones(self.in_channels))\n",
        "        self.lambda_tplsp = Parameter(torch.ones(self.in_channels))\n",
        "\n",
        "        self.batch_norm = Norm(norm_type=\"batch\", in_channels=self.in_channels, affine=False)\n",
        "        self.instance_norm = Norm(norm_type=\"instance\", in_channels=self.in_channels, affine=False)\n",
        "        self.layer_norm = Norm(norm_type=\"layer\", in_channels=self.in_channels, affine=False)\n",
        "        self.graph_norm = GraphNorm(self.in_channels, affine=False)\n",
        "        self.temporal_norm = TemporalNorm(self.in_channels, affine=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lambda_softmax = F.softmax(torch.cat([self.lambda_batch.unsqueeze(0), self.lambda_instance.unsqueeze(0), self.lambda_layer.unsqueeze(0), self.lambda_graph.unsqueeze(0), self.lambda_temporal.unsqueeze(0)], dim=0), dim=0)\n",
        "        x = lambda_softmax[0]*self.batch_norm(x) + lambda_softmax[1]*self.instance_norm(x) + lambda_softmax[2]*self.layer_norm(x) + lambda_softmax[3]*self.graph_norm(x) + lambda_softmax[4]*self.temporal_norm(x)\n",
        "        \n",
        "        if self.weight is not None and self.bias is not None:\n",
        "            x = x * self.weight + self.bias\n",
        "\n",
        "        #Append each norm's weighting at every step to track evolution\n",
        "        self.caller_class.norm_weights.append(list(lambda_softmax.mean(-1)))  #Store the mean weight (across features) for each normalisation strategy\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}({self.in_channels})'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoMB4SNHG3qj"
      },
      "outputs": [],
      "source": [
        "class SpatialThenTemporalNorm(torch.nn.Module):\n",
        "    \"\"\"Applies a united spatial norm on the input and then applies a temporal\n",
        "    norm on the spatially normalized input\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        affine (bool, optional): If set to :obj:`True`, this module has\n",
        "            learnable affine parameters :math:`\\gamma` and :math:`\\beta`.\n",
        "            (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, affine=True):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        if (affine):\n",
        "            self.weight = Parameter(torch.ones(self.in_channels))\n",
        "            self.bias = Parameter(torch.zeros(self.in_channels))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.temporal_norm = TemporalNorm(self.in_channels, affine=True)\n",
        "        self.spatial_norm = UnitedNorm(self.in_channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.spatial_norm(x)\n",
        "        x = self.temporal_norm(x)\n",
        "\n",
        "        if self.weight is not None and self.bias is not None:\n",
        "            x = x * self.weight + self.bias\n",
        "\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}({self.in_channels})'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcrxmiwpG9MA"
      },
      "outputs": [],
      "source": [
        "class TemporalThenSpatialNorm(torch.nn.Module):\n",
        "    \"\"\"Applies a temporal norm on the input and then applies a united spatial\n",
        "    norm on the temporally normalized input\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        affine (bool, optional): If set to :obj:`True`, this module has\n",
        "            learnable affine parameters :math:`\\gamma` and :math:`\\beta`.\n",
        "            (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, affine=True):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        if (affine):\n",
        "            self.weight = Parameter(torch.ones(self.in_channels))\n",
        "            self.bias = Parameter(torch.zeros(self.in_channels))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.temporal_norm = TemporalNorm(self.in_channels, affine=True)\n",
        "        self.spatial_norm = UnitedNorm(self.in_channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.temporal_norm(x)\n",
        "        x = self.spatial_norm(x)\n",
        "\n",
        "        if self.weight is not None and self.bias is not None:\n",
        "            x = x * self.weight + self.bias\n",
        "\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}({self.in_channels})'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qZqBy_qFiH-"
      },
      "outputs": [],
      "source": [
        "class SpatioTemporalNorm(torch.nn.Module):\n",
        "    \"\"\"Applies a normalization of the specified type.\n",
        "\n",
        "    Args:\n",
        "        norm_type (string): type of normalization to be applied\n",
        "        in_channels (int): Size of each input sample.\n",
        "    \"\"\"\n",
        "    def __init__(self, norm_type, in_channels, caller_class, **kwargs):\n",
        "        super().__init__()\n",
        "        self.norm_type = norm_type\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        if norm_type == \"instance\":\n",
        "            self.norm = Norm(norm_type=\"instance\", in_channels=in_channels, **kwargs)\n",
        "        elif norm_type == \"batch\":\n",
        "            self.norm = Norm(norm_type=\"batch\", in_channels=in_channels, **kwargs)\n",
        "        elif norm_type == \"layer\":\n",
        "            self.norm = Norm(norm_type=\"layer\", in_channels=in_channels, **kwargs)\n",
        "        elif norm_type == \"graph\":\n",
        "            self.norm = GraphNorm(in_channels=in_channels, **kwargs)\n",
        "        elif norm_type == \"united\":\n",
        "            self.norm = UnitedNorm(in_channels=in_channels, caller_class=caller_class, **kwargs)\n",
        "        elif norm_type == \"temporal\":\n",
        "            self.norm = TemporalNorm(in_channels=in_channels, **kwargs)\n",
        "        elif norm_type == \"united_temporal\":\n",
        "            self.norm = UnitedTemporalNorm(in_channels=in_channels, caller_class=caller_class, **kwargs)\n",
        "        elif norm_type == \"spatial_then_temporal\":\n",
        "            self.norm = SpatialThenTemporalNorm(in_channels=in_channels, **kwargs)\n",
        "        elif norm_type == \"temporal_then_spatial\":\n",
        "            self.norm = TemporalThenSpatialNorm(in_channels=in_channels, **kwargs)\n",
        "        elif norm_type == 'none':\n",
        "            self.norm = torch.nn.Identity()\n",
        "        else:\n",
        "            raise ValueError(\"Please choose one of the following Norm Types: instance, batch, layer, graph, united, temporal, united_temporal, spatial_then_temporal, temporal_then_spatial, none\")            \n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        return self.norm(x)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}({self.norm_type}, {self.in_channels})'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SpatioTemporalNorm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
